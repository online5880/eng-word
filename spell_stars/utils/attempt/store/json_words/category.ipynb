{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word count by category:\n",
      "Category 0: 22 words\n",
      "Category 1: 7 words\n",
      "Category 2: 8 words\n",
      "Category 3: 12 words\n",
      "Category 4: 7 words\n",
      "Category 5: 6 words\n",
      "Category 6: 22 words\n",
      "Category 7: 7 words\n",
      "Category 8: 8 words\n",
      "Category 9: 6 words\n",
      "Category 10: 6 words\n",
      "Category 11: 12 words\n",
      "Category 12: 6 words\n",
      "Category 13: 9 words\n",
      "Category 14: 4 words\n",
      "Category 15: 8 words\n",
      "Category 16: 7 words\n",
      "Category 17: 8 words\n",
      "Category 18: 6 words\n",
      "Category 19: 10 words\n",
      "Category 20: 13 words\n",
      "Category 21: 8 words\n",
      "Category 22: 9 words\n",
      "Category 23: 6 words\n",
      "Category 24: 11 words\n",
      "Category 25: 8 words\n",
      "Category 26: 7 words\n",
      "Category 27: 7 words\n",
      "Category 28: 9 words\n",
      "Category 29: 7 words\n",
      "Category 30: 7 words\n",
      "Category 31: 9 words\n",
      "Category 32: 12 words\n",
      "Category 33: 7 words\n",
      "Category 34: 15 words\n",
      "Category 35: 6 words\n",
      "Category 36: 8 words\n",
      "Category 37: 6 words\n",
      "Category 38: 8 words\n",
      "Category 39: 13 words\n",
      "Category 40: 9 words\n",
      "Category 41: 5 words\n",
      "Category 42: 17 words\n",
      "Category 43: 9 words\n",
      "Category 44: 5 words\n",
      "Category 45: 5 words\n",
      "Category 46: 7 words\n",
      "Category 47: 19 words\n",
      "Category 48: 7 words\n",
      "Category 49: 7 words\n",
      "Category 50: 10 words\n",
      "Category 51: 9 words\n",
      "Category 52: 3 words\n",
      "Category 53: 6 words\n",
      "Category 54: 6 words\n",
      "Category 55: 7 words\n",
      "Category 56: 7 words\n",
      "Category 57: 9 words\n",
      "Category 58: 13 words\n",
      "Category 59: 9 words\n",
      "Category 60: 6 words\n",
      "Category 61: 10 words\n",
      "Category 62: 8 words\n",
      "Category 63: 5 words\n",
      "Category 64: 5 words\n",
      "Category 65: 9 words\n",
      "Category 66: 9 words\n",
      "Category 67: 8 words\n",
      "Category 68: 11 words\n",
      "Category 69: 8 words\n",
      "Category 70: 9 words\n",
      "Category 71: 7 words\n",
      "Category 72: 7 words\n",
      "Category 73: 9 words\n",
      "Category 74: 6 words\n",
      "Category 75: 13 words\n",
      "Category 76: 8 words\n",
      "Category 77: 5 words\n",
      "Category 78: 14 words\n",
      "Category 79: 9 words\n",
      "Category 80: 4 words\n",
      "Category 81: 9 words\n",
      "Category 82: 12 words\n",
      "Category 83: 5 words\n",
      "Category 84: 9 words\n",
      "Category 85: 12 words\n",
      "Category 86: 23 words\n",
      "Category 87: 5 words\n",
      "Category 88: 6 words\n",
      "Category 89: 6 words\n",
      "Category 90: 4 words\n",
      "Category 91: 8 words\n",
      "Category 92: 9 words\n",
      "Category 93: 10 words\n",
      "Category 94: 21 words\n",
      "Category 95: 17 words\n",
      "Category 96: 4 words\n",
      "Category 97: 50 words\n",
      "Category 98: 9 words\n",
      "Category 99: 6 words\n",
      "Category 100: 10 words\n",
      "Category 101: 7 words\n",
      "Category 102: 7 words\n",
      "Category 103: 9 words\n",
      "Category 104: 13 words\n",
      "Category 105: 13 words\n",
      "Category 106: 4 words\n",
      "Category 107: 13 words\n",
      "Category 108: 6 words\n",
      "Category 109: 14 words\n",
      "Category 110: 7 words\n",
      "Category 111: 6 words\n",
      "Category 112: 10 words\n",
      "Category 113: 7 words\n",
      "Category 114: 9 words\n",
      "Category 115: 8 words\n",
      "Category 116: 10 words\n",
      "Category 117: 8 words\n",
      "Category 118: 8 words\n",
      "Category 119: 7 words\n",
      "Category 120: 10 words\n",
      "Category 121: 8 words\n",
      "Category 122: 7 words\n",
      "Category 123: 18 words\n",
      "Category 124: 10 words\n",
      "Category 125: 5 words\n",
      "Category 126: 6 words\n",
      "Category 127: 7 words\n",
      "Category 128: 6 words\n",
      "Category 129: 13 words\n",
      "Category 130: 6 words\n",
      "Category 131: 5 words\n",
      "Category 132: 7 words\n",
      "Category 133: 9 words\n",
      "Category 134: 4 words\n",
      "Category 135: 13 words\n",
      "Category 136: 10 words\n",
      "Category 137: 7 words\n",
      "Category 138: 8 words\n",
      "Category 139: 11 words\n",
      "Category 140: 7 words\n",
      "Category 141: 6 words\n",
      "Category 142: 2 words\n",
      "Category 143: 6 words\n",
      "Category 144: 14 words\n",
      "Category 145: 8 words\n",
      "Category 146: 9 words\n",
      "Category 147: 11 words\n",
      "Category 148: 4 words\n",
      "Category 149: 5 words\n",
      "Category 150: 9 words\n",
      "Category 151: 11 words\n",
      "Category 152: 3 words\n",
      "Category 153: 7 words\n",
      "Category 154: 8 words\n",
      "Category 155: 8 words\n",
      "Category 156: 8 words\n",
      "Category 157: 11 words\n",
      "Category 158: 6 words\n",
      "Category 159: 7 words\n",
      "Category 160: 5 words\n",
      "Category 161: 10 words\n",
      "Category 162: 5 words\n",
      "Category 163: 9 words\n",
      "Category 164: 5 words\n",
      "Category 165: 4 words\n",
      "Category 166: 7 words\n",
      "Category 167: 5 words\n",
      "Category 168: 10 words\n",
      "Category 169: 5 words\n",
      "Category 170: 7 words\n",
      "Category 171: 9 words\n",
      "Category 172: 9 words\n",
      "Category 173: 14 words\n",
      "Category 174: 3 words\n",
      "Category 175: 13 words\n",
      "Category 176: 8 words\n",
      "Category 177: 5 words\n",
      "Category 178: 7 words\n",
      "Category 179: 5 words\n",
      "Category 180: 6 words\n",
      "Category 181: 6 words\n",
      "Category 182: 6 words\n",
      "Category 183: 13 words\n",
      "Category 184: 8 words\n",
      "Category 185: 11 words\n",
      "Category 186: 7 words\n",
      "Category 187: 7 words\n",
      "Category 188: 8 words\n",
      "Category 189: 6 words\n",
      "Category 190: 5 words\n",
      "Category 191: 18 words\n",
      "Category 192: 11 words\n",
      "Category 193: 4 words\n",
      "Category 194: 11 words\n",
      "Category 195: 3 words\n",
      "Category 196: 12 words\n",
      "Category 197: 9 words\n",
      "Category 198: 6 words\n",
      "Category 199: 6 words\n",
      "Category 200: 9 words\n",
      "Category 201: 11 words\n",
      "Category 202: 4 words\n",
      "Category 203: 8 words\n",
      "Category 204: 3 words\n",
      "Category 205: 7 words\n",
      "Category 206: 5 words\n",
      "Category 207: 9 words\n",
      "Category 208: 7 words\n",
      "Category 209: 6 words\n",
      "Category 210: 9 words\n",
      "Category 211: 7 words\n",
      "Category 212: 5 words\n",
      "Category 213: 7 words\n",
      "Category 214: 9 words\n",
      "Category 215: 6 words\n",
      "Category 216: 3 words\n",
      "Category 217: 7 words\n",
      "Category 218: 5 words\n",
      "Category 219: 8 words\n",
      "Category 220: 8 words\n",
      "Category 221: 6 words\n",
      "Category 222: 14 words\n",
      "Category 223: 6 words\n",
      "Category 224: 7 words\n",
      "Category 225: 7 words\n",
      "Category 226: 7 words\n",
      "Category 227: 5 words\n",
      "Category 228: 12 words\n",
      "Category 229: 7 words\n",
      "\n",
      "Clustered vocabulary saved to ./clustered_wordbook.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import Counter\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "np.random.seed(42)\n",
    "\n",
    "# Step 1: Load JSON files containing vocabulary\n",
    "def load_words_from_json(directory_path):\n",
    "    word_dict = {}\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            with open(os.path.join(directory_path, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "                data = json.load(file)\n",
    "                for item in data:\n",
    "                    word = item[\"단어\"].replace(\"\\n\", \"\")\n",
    "                    meaning = item[\"단어 뜻\"].replace(\"\\n\", \"\")\n",
    "                    \n",
    "                    # If word is already in word_dict\n",
    "                    if word in word_dict:\n",
    "                        existing_item = word_dict[word]\n",
    "                        \n",
    "                        # Add meaning if not already present\n",
    "                        if meaning not in existing_item[\"meanings\"]:\n",
    "                            existing_item[\"meanings\"].append(meaning)\n",
    "                        \n",
    "                        # Add example sentences in both English and Korean\n",
    "                        example = {\n",
    "                            \"english\": item.get(\"예문\", \"\").replace(\"\\n\", \"\"),\n",
    "                            \"korean\": item.get(\"예문 뜻\", \"\").replace(\"\\n\", \"\")\n",
    "                        }\n",
    "                        if example not in existing_item[\"examples\"]:\n",
    "                            existing_item[\"examples\"].append(example)\n",
    "                    else:\n",
    "                        # Initialize new word entry\n",
    "                        word_dict[word] = {\n",
    "                            \"meanings\": [meaning],\n",
    "                            \"part_of_speech\": item.get(\"품사\", \"\"),\n",
    "                            \"examples\": [{\n",
    "                                \"english\": item.get(\"예문\", \"\").replace(\"\\n\", \"\"),\n",
    "                                \"korean\": item.get(\"예문 뜻\", \"\").replace(\"\\n\", \"\")\n",
    "                            }]\n",
    "                        }\n",
    "    return word_dict\n",
    "\n",
    "# Step 2: Vectorize words\n",
    "def vectorize_words(word_list):\n",
    "    model = SentenceTransformer('all-mpnet-base-v2', device='cpu')  # Initialize model\n",
    "    vectors = model.encode(word_list)\n",
    "    return vectors\n",
    "\n",
    "# Step 3: Cluster words using DBSCAN\n",
    "def cluster_words(word_list, vectors, eps=0.35, min_samples=2):\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')\n",
    "    clusters = dbscan.fit_predict(vectors)\n",
    "    \n",
    "    # Store clustering results\n",
    "    clustered_words = {}\n",
    "    noise = []\n",
    "    for idx, cluster_id in enumerate(clusters):\n",
    "        if cluster_id != -1:  # If part of a cluster\n",
    "            if cluster_id not in clustered_words:\n",
    "                clustered_words[cluster_id] = []\n",
    "            clustered_words[cluster_id].append((word_list[idx], vectors[idx]))\n",
    "        else:  # If classified as noise\n",
    "            noise.append((word_list[idx], vectors[idx]))\n",
    "    \n",
    "    return clustered_words, noise\n",
    "\n",
    "# Step 4: Assign noise words to nearest clusters individually\n",
    "def assign_noise_to_nearest_cluster(clustered_words, noise):\n",
    "    if not noise:\n",
    "        return clustered_words\n",
    "\n",
    "    # Compare each noise word with clusters to find the most similar cluster\n",
    "    for noise_word, noise_vector in noise:\n",
    "        max_similarity = -1\n",
    "        best_cluster_id = None\n",
    "\n",
    "        # Calculate similarity with each cluster\n",
    "        for cluster_id, words_with_vectors in clustered_words.items():\n",
    "            cluster_vectors = np.array([vec for _, vec in words_with_vectors])\n",
    "            similarities = cosine_similarity([noise_vector], cluster_vectors)\n",
    "            avg_similarity = np.mean(similarities)\n",
    "\n",
    "            # Find cluster with highest similarity\n",
    "            if avg_similarity > max_similarity:\n",
    "                max_similarity = avg_similarity\n",
    "                best_cluster_id = cluster_id\n",
    "\n",
    "        # Add noise word to the most similar cluster\n",
    "        if best_cluster_id is not None:\n",
    "            clustered_words[best_cluster_id].append((noise_word, noise_vector))\n",
    "    \n",
    "    return clustered_words\n",
    "\n",
    "# Step 5: Assign categories and save refined clusters\n",
    "def refine_clusters_auto(clustered_words, word_dict):\n",
    "    refined_clusters = {}\n",
    "    category_counts = Counter()  # Word count by category\n",
    "    category_id = 0\n",
    "\n",
    "    # Assign categories without further re-clustering\n",
    "    for cluster_id, words_with_vectors in clustered_words.items():\n",
    "        for word, _ in words_with_vectors:\n",
    "            word_dict[word][\"category\"] = int(category_id)\n",
    "            refined_clusters[word] = word_dict[word]\n",
    "        category_counts[category_id] = len(words_with_vectors)\n",
    "        category_id += 1\n",
    "\n",
    "    # Print word count by category\n",
    "    print(\"\\nWord count by category:\")\n",
    "    for category, count in category_counts.items():\n",
    "        print(f\"Category {category}: {count} words\")\n",
    "\n",
    "    return refined_clusters\n",
    "\n",
    "# Step 6: Save refined clusters to JSON\n",
    "def save_refined_clusters_to_single_json(refined_clusters, output_path):\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(refined_clusters, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Execution\n",
    "directory_path = \"./data\"  # Path to folder with JSON files\n",
    "output_path = \"./clustered_wordbook.json\"  # Path to save clustered output\n",
    "word_dict = load_words_from_json(directory_path)\n",
    "word_list = list(word_dict.keys())\n",
    "vectors = vectorize_words(word_list)  # Vectorize words\n",
    "clustered_words, noise = cluster_words(word_list, vectors, eps=0.35, min_samples=2)\n",
    "clustered_words_with_noise = assign_noise_to_nearest_cluster(clustered_words, noise)\n",
    "refined_clusters = refine_clusters_auto(clustered_words_with_noise, word_dict)\n",
    "save_refined_clusters_to_single_json(refined_clusters, output_path)\n",
    "\n",
    "print(f\"\\nClustered vocabulary saved to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
