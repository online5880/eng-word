{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클러스터링된 단어장이 ./clustered_wordbook.json 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "np.random.seed(42)\n",
    "\n",
    "# Step 1: 단어장 JSON 파일 불러오기\n",
    "def load_words_from_json(directory_path):\n",
    "    word_dict = {}\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            with open(os.path.join(directory_path, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "                data = json.load(file)\n",
    "                for item in data:\n",
    "                    word = item[\"단어\"]\n",
    "                    meaning = item[\"단어 뜻\"]\n",
    "                    if word in word_dict:\n",
    "                        existing_item = word_dict[word]\n",
    "                        # 단어 뜻이 동일한 경우 중복 제거\n",
    "                        if meaning not in existing_item[\"단어 뜻\"]:\n",
    "                            # 단어 뜻을 리스트로 확장하여 추가\n",
    "                            if not isinstance(existing_item[\"단어 뜻\"], list):\n",
    "                                existing_item[\"단어 뜻\"] = [existing_item[\"단어 뜻\"]]\n",
    "                            existing_item[\"단어 뜻\"].append(meaning)\n",
    "                        # 예문과 예문 뜻을 리스트로 확장하여 추가\n",
    "                        for field in [\"예문\", \"예문 뜻\"]:\n",
    "                            if field in item:\n",
    "                                if not isinstance(existing_item[field], list):\n",
    "                                    existing_item[field] = [existing_item[field]]\n",
    "                                existing_item[field].append(item[field])\n",
    "                    else:\n",
    "                        word_dict[word] = item  # 처음 등장하는 단어는 그대로 추가\n",
    "    return list(word_dict.values())\n",
    "\n",
    "# Step 2: 단어를 벡터화\n",
    "def vectorize_words(word_list):\n",
    "    model = SentenceTransformer('all-mpnet-base-v2', device='cpu')  # 모델 초기화\n",
    "    words = [word[\"단어\"] for word in word_list]\n",
    "    vectors = model.encode(words)\n",
    "    return vectors\n",
    "\n",
    "# Step 3: DBSCAN으로 군집화\n",
    "def cluster_words(word_list, vectors, eps=0.35, min_samples=2):\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')\n",
    "    clusters = dbscan.fit_predict(vectors)\n",
    "    clustered_words = {}\n",
    "    for idx, cluster_id in enumerate(clusters):\n",
    "        if cluster_id != -1:  # 노이즈 항목 제외\n",
    "            if cluster_id not in clustered_words:\n",
    "                clustered_words[cluster_id] = []\n",
    "            clustered_words[cluster_id].append(word_list[idx])\n",
    "    return clustered_words\n",
    "\n",
    "# Step 4: 주제별로 세분화\n",
    "def refine_clusters_auto(clustered_words):\n",
    "    refined_clusters = {}\n",
    "    for cluster_id, words in clustered_words.items():\n",
    "        topic_name = f\"category_{cluster_id}\"  # 자동으로 생성된 주제명\n",
    "        refined_clusters[topic_name] = words\n",
    "    return refined_clusters\n",
    "\n",
    "# Step 5: JSON 파일로 저장\n",
    "def save_refined_clusters_to_single_json(refined_clusters, output_path):\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(refined_clusters, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 실행\n",
    "directory_path = \"./data\"  # JSON 파일들이 있는 폴더 경로 설정\n",
    "output_path = \"./clustered_wordbook.json\"  # 군집화 결과를 저장할 파일 경로 설정\n",
    "word_list = load_words_from_json(directory_path)\n",
    "vectors = vectorize_words(word_list)  # 단어 자체를 벡터화\n",
    "clustered_words = cluster_words(word_list, vectors, eps=0.35, min_samples=2)\n",
    "refined_clusters = refine_clusters_auto(clustered_words)\n",
    "save_refined_clusters_to_single_json(refined_clusters, output_path)\n",
    "\n",
    "print(f\"클러스터링된 단어장이 {output_path} 파일에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/venv/lib/python3.8/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클러스터링된 단어장이 ./clustered_wordbook.json 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "np.random.seed(42)\n",
    "\n",
    "# Step 1: 단어장 JSON 파일 불러오기\n",
    "def load_words_from_json(directory_path):\n",
    "    word_dict = {}\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            with open(os.path.join(directory_path, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "                data = json.load(file)\n",
    "                for item in data:\n",
    "                    word = item[\"단어\"]\n",
    "                    meaning = item[\"단어 뜻\"]\n",
    "                    \n",
    "                    # 단어가 이미 word_dict에 있는 경우\n",
    "                    if word in word_dict:\n",
    "                        existing_item = word_dict[word]\n",
    "                        \n",
    "                        # 단어 뜻 중복 검사 후 추가\n",
    "                        if meaning not in existing_item[\"단어 뜻\"]:\n",
    "                            existing_item[\"단어 뜻\"].append(meaning)\n",
    "                        \n",
    "                        # 예문과 예문 뜻 중복 검사 없이 추가\n",
    "                        for field in [\"예문\", \"예문 뜻\"]:\n",
    "                            if field in item:\n",
    "                                existing_item[field].append(item[field])\n",
    "                    else:\n",
    "                        # 새로운 단어의 경우 리스트로 초기화하여 저장\n",
    "                        word_dict[word] = {\n",
    "                            \"단어\": word,\n",
    "                            \"단어 뜻\": [meaning],\n",
    "                            \"품사\": item.get(\"품사\", \"\"),\n",
    "                            \"예문\": [item.get(\"예문\", \"\")],\n",
    "                            \"예문 뜻\": [item.get(\"예문 뜻\", \"\")]\n",
    "                        }\n",
    "    return list(word_dict.values())\n",
    "\n",
    "# Step 2: 단어를 벡터화\n",
    "def vectorize_words(word_list):\n",
    "    model = SentenceTransformer('all-mpnet-base-v2', device='cpu')  # 모델 초기화\n",
    "    words = [word[\"단어\"] for word in word_list]\n",
    "    vectors = model.encode(words)\n",
    "    return vectors\n",
    "\n",
    "# Step 3: DBSCAN으로 군집화\n",
    "def cluster_words(word_list, vectors, eps=0.35, min_samples=2):\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')\n",
    "    clusters = dbscan.fit_predict(vectors)\n",
    "    clustered_words = {}\n",
    "    for idx, cluster_id in enumerate(clusters):\n",
    "        if cluster_id != -1:  # 노이즈 항목 제외\n",
    "            if cluster_id not in clustered_words:\n",
    "                clustered_words[cluster_id] = []\n",
    "            clustered_words[cluster_id].append(word_list[idx])\n",
    "    return clustered_words\n",
    "\n",
    "# Step 4: 주제별로 세분화\n",
    "def refine_clusters_auto(clustered_words):\n",
    "    refined_clusters = {}\n",
    "    for cluster_id, words in clustered_words.items():\n",
    "        topic_name = f\"category_{cluster_id}\"  # 자동으로 생성된 주제명\n",
    "        refined_clusters[topic_name] = words\n",
    "    return refined_clusters\n",
    "\n",
    "# Step 5: JSON 파일로 저장\n",
    "def save_refined_clusters_to_single_json(refined_clusters, output_path):\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(refined_clusters, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 실행\n",
    "directory_path = \"./data\"  # JSON 파일들이 있는 폴더 경로 설정\n",
    "output_path = \"./clustered_wordbook.json\"  # 군집화 결과를 저장할 파일 경로 설정\n",
    "word_list = load_words_from_json(directory_path)\n",
    "vectors = vectorize_words(word_list)  # 단어 자체를 벡터화\n",
    "clustered_words = cluster_words(word_list, vectors, eps=0.35, min_samples=2)\n",
    "refined_clusters = refine_clusters_auto(clustered_words)\n",
    "save_refined_clusters_to_single_json(refined_clusters, output_path)\n",
    "\n",
    "print(f\"클러스터링된 단어장이 {output_path} 파일에 저장되었습니다.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
