{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어, 단어 뜻, 품사, 예문, 예문 뜻 추출\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:54<00:00,  4.99s/it]\n"
     ]
    }
   ],
   "source": [
    "#  최종 코드\n",
    "import pdfplumber\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "if not os.path.exists(\"./static\"):\n",
    "    os.mkdir(\"./static\")\n",
    "    \n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# 디렉토리 설정\n",
    "PDF_DIR = \"/Users/mane/Documents/프로젝트/eng-word/data/pdf\" # pdf 넣을 폴더\n",
    "OUT_DIR = \"./json_words/data\" # 변환된 json들이 들어갈 폴더\n",
    "\n",
    "def extract_lesson_titles(page): # 페이지에서 \"Lesson\" 제목을 추출하는 함수\n",
    "    text = page.extract_text()  # 페이지의 텍스트를 추출\n",
    "    lesson_titles = re.findall(r'Lesson \\d+\\. (.+?)\\s*\\n', text)  # \"Lesson\" 번호와 제목을 정규식으로 찾음\n",
    "    return lesson_titles\n",
    "\n",
    "def extract_data_from_table(row): # 테이블의 행에서 예문과 예문 뜻을 분리하는 함수\n",
    "    example_sentence, example_meaning = \"\", \"\"\n",
    "    if not isinstance(row[6], str):  # Check if row[6] is a string\n",
    "        return {}  # Return an empty dictionary if not a string\n",
    "\n",
    "    if re.search(r'[A-Za-z]+.*?[.!?]\\s+[가-힣]', row[6]):  # 영어와 한글이 섞인 패턴을 찾음\n",
    "        parts = re.split(r'(?<=[.!?])\\s+(?=[가-힣])', row[6], 1)  # 영어 문장과 한글 뜻을 분리\n",
    "    elif re.search(r'[A-Za-z]+\\s+[가-힣]', row[6]):\n",
    "        parts = re.split(r'(?<=[A-Za-z])\\s+(?=[가-힣])', row[6], 1)  # 영어 단어와 한글 뜻을 분리\n",
    "    elif \"\\n\" in row[6]:\n",
    "        parts = row[6].split(\"\\n\", 1)  # 개행으로 구분된 경우 분리\n",
    "    else:\n",
    "        parts = [row[6].strip(), \"\"]  # 기본적으로 공백 제거 후 할당\n",
    "\n",
    "    example_sentence = remove_newlines(parts[0].strip())  # 예문에서 개행 제거\n",
    "    mixed = extract_sentences_with_korean(example_sentence)  # 예문에서 한국어 문장을 추출\n",
    "    example_meaning = remove_newlines(parts[1].strip()) if len(parts) > 1 else \"\"  # 예문 뜻에서 개행 제거\n",
    "    \n",
    "    # 예문과 예문 뜻에서 한국어 문장이 섞여 있는 경우 처리\n",
    "    if example_meaning.startswith('씨'):\n",
    "        concat = example_sentence + example_meaning\n",
    "        example_meaning = extract_sentences_with_korean(concat)[0]\n",
    "        example_sentence = concat.replace(example_meaning, '')\n",
    "    \n",
    "    if len(mixed) > 0: \n",
    "        if mixed[0] in example_sentence:\n",
    "            example_sentence = example_sentence.replace(mixed[0], '')\n",
    "            example_meaning = mixed[0] + ' ' + example_meaning\n",
    "\n",
    "    # 품사와 단어 뜻을 정규식으로 추출\n",
    "    pos_match = re.search(r'(\\w+)\\)\\s*', row[3])\n",
    "    if pos_match:\n",
    "        part_of_speech = pos_match.group(1)  # 품사 추출\n",
    "        meaning_text = re.sub(r'\\s*\\(\\w+\\)\\s*', '', row[3]).strip()  # 뜻에서 품사 제거\n",
    "    else:\n",
    "        part_of_speech = \"\"\n",
    "        meaning_text = row[3].strip()\n",
    "\n",
    "    # 반환할 단어 정보 사전 생성\n",
    "    return {\n",
    "        \"단어\": remove_newlines(row[0].strip()),\n",
    "        \"단어 뜻\": meaning_text.replace(part_of_speech + ')', '').strip(),\n",
    "        \"품사\": part_of_speech,\n",
    "        \"예문\": example_sentence.strip(),\n",
    "        \"예문 뜻\": example_meaning.strip(),\n",
    "    }\n",
    "\n",
    "def remove_newlines(text):  # 문자열에서 개행 문자를 제거하는 함수\n",
    "    return text.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "\n",
    "def extract_sentences_with_korean(text):  # 영문장에서 한국어가 존재시 재 추출하는 함수\n",
    "    sentences = re.split(r'(?<=[.?!])\\s+', text)\n",
    "    korean_sentences = [sentence for sentence in sentences if re.search(r'[가-힣]', sentence)]\n",
    "    return korean_sentences\n",
    "\n",
    "def safe_filename(filename):  # 파일 이름에서 안전하지 않은 문자를 제거하는 함수\n",
    "    return re.sub(r'[\\/\\:*?\"<>|]', \"\", filename)\n",
    "\n",
    "def pdf_to_json(pdf_path, output_directory):  # beyond.pdf를 제외한 pdf용 : pdfplumber를 사용하여 PDF를 JSON으로 변환하는 함수\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_number, page in enumerate(pdf.pages):\n",
    "            lesson_titles = extract_lesson_titles(page)  # \"Lesson\" 제목 추출\n",
    "            title_text = \" - \".join(lesson_titles)  # 제목을 파일명으로 변환\n",
    "            safe_title_text = safe_filename(title_text)  # 안전한 파일명 변환\n",
    "            data = []\n",
    "            tables = page.extract_tables()  # 페이지에서 테이블 추출\n",
    "            for table in tables:\n",
    "                for row in table:\n",
    "                    if len(row) > 0:\n",
    "                        entry = extract_data_from_table(row)  # 테이블 행을 데이터로 변환\n",
    "                        if any(entry.values()):\n",
    "                            data.append(entry)\n",
    "            # JSON 파일로 저장\n",
    "            output_file = os.path.join(output_directory, f\"{os.path.basename(pdf_path).replace('.pdf', '')}_page_{page_number + 1}_{safe_title_text}.json\")\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as json_file:\n",
    "                json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "            # print(f\"Parsed data for {pdf_path} page {page_number + 1}:\", data)\n",
    "\n",
    "def clean_text(text):  # 텍스트의 양끝 공백을 제거하는 함수\n",
    "    return text.strip()\n",
    "\n",
    "def format_sentence(sentence):  # 예문 형식을 정리하는 함수\n",
    "    if re.search(r'\\s햄', sentence):\n",
    "        sentence = sentence.replace(\".  햄\", \"\\n햄\")\n",
    "    return sentence\n",
    "\n",
    "def extract_words_from_pdf_by_page(pdf_path, output_dir=\"output_pages\"):  # beyond.pdf용 : PyMuPDF를 사용하여 페이지별로 PDF 단어를 추출하는 함수\n",
    "    document = fitz.open(pdf_path)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for page_num in range(document.page_count):\n",
    "        page = document.load_page(page_num)\n",
    "        text = format_sentence(page.get_text().replace('\"', ''))\n",
    "        text = text.replace(\". 나\", \"\\n나\").replace(\"\\n는\", \"는\").replace(\". 우\", \"\\n우\").replace(\"\\n만\", \"만\").replace(\"\\n다\", \"다\").replace(\"\\n있다\", \"있다\").replace(\"\\n은편에\", \"은편에\").replace(\"그녀는\", \"\\n그녀는\").replace(\"학습\", \"\\n학습\").replace(\"\\nstudents\", \"students\")\n",
    "        \n",
    "        # Lesson 제목을 뽑아서 파일 이름에 포함\n",
    "        lesson_title = re.search(r'Lesson \\d+\\. (.+?)\\s*\\n', text)\n",
    "        title_text = lesson_title.group(1) if lesson_title else f\"Page_{page_num + 1}\"\n",
    "        safe_title_text = safe_filename(title_text)\n",
    "\n",
    "        # 단어 정보 추출 패턴 정의\n",
    "        pattern = r\"(?P<단어>\\w+)\\s+(?P<품사>[a-z.]+)\\s+(?P<단어_뜻>.+?)\\n(?P<예문>.+?)\\n(?P<예문_뜻>.+?)\\n\"\n",
    "        if page_num == 12 or page_num == 23:\n",
    "            pattern = r\"(?P<단어>\\w+(?:\\s\\w+)*)\\s+(?P<품사>[a-z.]+)\\s+(?P<단어_뜻>.+?)\\n(?P<예문>.+?)\\n(?P<예문_뜻>.+?)\\n\"\n",
    "        matches = re.finditer(pattern, text, re.MULTILINE)\n",
    "        \n",
    "        page_words = []\n",
    "        for match in matches:\n",
    "            word_info = {\n",
    "                \"단어\": clean_text(match.group(\"단어\")),\n",
    "                \"단어 뜻\": clean_text(match.group(\"단어_뜻\")),\n",
    "                \"품사\": clean_text(match.group(\"품사\")),\n",
    "                \"예문\": clean_text(match.group(\"예문\")),\n",
    "                \"예문 뜻\": clean_text(match.group(\"예문_뜻\"))\n",
    "            }\n",
    "            if word_info[\"단어\"] and word_info[\"예문\"]:\n",
    "                page_words.append(word_info)\n",
    "        \n",
    "        # JSON 파일 저장\n",
    "        output_file = os.path.join(output_dir, f\"{os.path.basename(pdf_path).replace('.pdf', '')}_page_{page_num + 1}_{safe_title_text}.json\")\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as json_file:\n",
    "            json.dump(page_words, json_file, ensure_ascii=False, indent=4)\n",
    "    \n",
    "def process_pdfs(pdf_directory, output_directory, include_beyond=False, ):\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    for pdf_file in tqdm(os.listdir(pdf_directory)):\n",
    "        pdf_path = os.path.join(pdf_directory, pdf_file)\n",
    "        if pdf_file.endswith(\".pdf\"):\n",
    "            if pdf_file == \"T9EE70U04.pdf\" and include_beyond: # beyond 전용\n",
    "                print(f\"{pdf_file}를 처리하는 중...\")\n",
    "                extract_words_from_pdf_by_page(pdf_path, output_dir=output_directory)\n",
    "            else:\n",
    "                pdf_to_json(pdf_path, output_directory)\n",
    "\n",
    "# PDF 파일 처리\n",
    "process_pdfs(PDF_DIR, OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 / 예문 JSON 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어들이 ./json_words/combined/combined_words.json 파일에 저장되었습니다.\n",
      "예문들이 ./json_words/combined/combined_examples.json 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# JSON 파일들이 있는 디렉토리를 정의합니다\n",
    "JSON_DIR = \"./json_words/data\"  # JSON 파일들이 있는 폴더 경로로 변경하세요\n",
    "OUT_WORD_JSON_FILE = \"./json_words/combined/combined_words.json\"\n",
    "OUT_EXAMPLE_JSON_FILE = \"./json_words/combined/combined_examples.json\"\n",
    "\n",
    "if not os.path.exists(\"./json_words/combined\"):\n",
    "    os.makedirs(\"./json_words/combined\")\n",
    "\n",
    "# 단어와 예문을 저장할 리스트\n",
    "words_list = []\n",
    "examples_list = []\n",
    "\n",
    "# 디렉토리 내 모든 JSON 파일을 순회합니다\n",
    "for filename in os.listdir(JSON_DIR):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(JSON_DIR, filename)\n",
    "        \n",
    "        # JSON 파일을 열고 로드합니다\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "            data = json.load(json_file)\n",
    "            \n",
    "            # 각 항목에서 \"단어\"와 \"예문\" 필드를 추출합니다\n",
    "            for entry in data:\n",
    "                word = entry.get(\"단어\", \"\")\n",
    "                example = entry.get(\"예문\", \"\")\n",
    "                \n",
    "                # 단어 또는 예문이 있는 경우 각각의 리스트에 추가합니다\n",
    "                if word:\n",
    "                    words_list.append(word)\n",
    "                if example and all(not char.isdigit() and char not in '가-힣' for char in example):  # 예문에 한글이 없는 경우만 추가\n",
    "                    examples_list.append(example)\n",
    "\n",
    "# 단어들을 새로운 JSON 파일로 저장합니다\n",
    "with open(OUT_WORD_JSON_FILE, \"w\", encoding=\"utf-8\") as output_words_json:\n",
    "    json.dump(words_list, output_words_json, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 예문들을 새로운 JSON 파일로 저장합니다\n",
    "with open(OUT_EXAMPLE_JSON_FILE, \"w\", encoding=\"utf-8\") as output_examples_json:\n",
    "    json.dump(examples_list, output_examples_json, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"단어들이 {OUT_WORD_JSON_FILE} 파일에 저장되었습니다.\")\n",
    "print(f\"예문들이 {OUT_EXAMPLE_JSON_FILE} 파일에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
